# Assignment 1 â€” Flower Classification (CRISP-DM, Transfer Learning)

A lean, reproducible image classification project on the Kaggle **Flowers Recognition** dataset using **TensorFlow/Keras**. It follows the **CRISP-DM** methodology endâ€‘toâ€‘end, is optimized for **limited compute**, and ships with **explainability (Grad-CAM)** and **deployment (SavedModel & TFLite)**.

---

## ğŸ“¦ Quick Start

> You can run the whole pipeline from the provided notebook cells (Chunks 1â€“7). The project is modular; each chunk is selfâ€‘contained and documented.

```bash
# (Optional) Install deps in your environment
pip install -U kagglehub tensorflow pandas scikit-learn matplotlib pillow tqdm gradio

# Start Jupyter and open the notebook
jupyter notebook
```

**Data download (in-notebook):**
```python
import kagglehub
path = kagglehub.dataset_download("alxmamaev/flowers-recognition")
print("Kaggle dataset path:", path)
```

---

## ğŸ—‚ï¸ Repository / Folder Structure

Suggested structure for your GitHub repo:

```
assignment-1/
â”œâ”€â”€ README.md                           # â† this file
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ FlowerClassification.ipynb      # your working notebook
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.keras                # trained best model (from Chunk 3/6)
â”‚   â””â”€â”€ id2label.json                   # label maps (Chunk 3)
â”‚       label2id.json
â”œâ”€â”€ export/                             # deployment artifacts
â”‚   â”œâ”€â”€ savedmodel/                     # TF SavedModel export (Chunk 6)
â”‚   â”œâ”€â”€ model_fp32.tflite               # TFLite FP32 (Chunk 6/6b)
â”‚   â””â”€â”€ model_float16.tflite            # TFLite float16 (preferred on CPU)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ flower_cli.py                   # tiny CLI for inference (Chunk 7)
â””â”€â”€ requirements.txt                    # optional pinning for reproducibility
```

> When you run the notebook, artifacts are created in `./models` and `./export`. Move them into your repo structure as shown above before committing.

---

## ğŸ§­ CRISP-DM Walkthrough & How to Run

Each chunk corresponds to a CRISPâ€‘DM phase and provides readyâ€‘toâ€‘run cells:

1) **Business & Data Understanding (Chunk 1)**  
   - Goal: accurate flower classification with low compute.  
   - Actions: environment setup, dataset download, quick EDA (class balance & samples).

2) **Data Preparation (Chunk 2)**  
   - Stratified **train/val/test** splits, efficient **tf.data** pipelines, **class weights**, and light **onâ€‘model augmentation**.

3) **Modeling (Chunk 3)**  
   - **Transfer learning** with **MobileNetV2 (default)** or **EfficientNetB0**.  
   - Twoâ€‘phase training: **warmâ€‘up (frozen backbone)** â†’ **fineâ€‘tune (top layers)**.  
   - Fixed LR handling (float LRs; `ReduceLROnPlateau` compatible).  
   - Artifacts saved: `models/best_model.keras`, `models/id2label.json`, `models/label2id.json`.

4) **Evaluation (Chunk 4)**  
   - Test **accuracy** & **Topâ€‘3**, **classification report**, **confusion matrices**, and **misclassification gallery**.

5) **Explainability (Chunk 5)**  
   - **Gradâ€‘CAM** utilities (final robust version) with gallery & singleâ€‘image helpers.  
   - Works reliably by **rebuilding the forward pass** for a single connected graph.

6) **Deployment â€” Exports (Chunk 6 & 6b)**  
   - Build a **fresh inferenceâ€‘only model** (no augmentation) and **copy weights**.  
   - Export **SavedModel** and convert to **TFLite** (FP32 + float16) with robust fallbacks.  
   - Parity checks via cosine similarity and TFLite sanity run.

7) **Deployment â€” Interfaces (Chunk 7)**  
   - **Unified predictor** (Keras & TFLite).  
   - **CLI** script (`scripts/flower_cli.py`) and an optional **Gradio** miniâ€‘app.

---

## ğŸ§ª Reproducibility

- Global seed: `42` (see `set_global_seed`).  
- Recommended versions (print cell provided in Chunk 6):
  - Python â‰¥ 3.10
  - TensorFlow â‰¥ 2.15 / Keras 3+
  - numpy, pandas, scikitâ€‘learn, matplotlib, pillow, tqdm, gradio
- Suggested `requirements.txt`:
```
tensorflow>=2.15
keras>=3.0.0
numpy
pandas
scikit-learn
matplotlib
pillow
tqdm
kagglehub
gradio
```

---

## ğŸ§  Model Choice & Training Process

- **Backbones:** MobileNetV2 (fast, ~3.5M params) or EfficientNetB0 (~5.3M).  
- **Why:** small yet strong on small datasets; ideal for CPUâ€‘only training.  
- **Process:** warmâ€‘up head with frozen backbone â†’ unfreeze top ~20% (BatchNorm frozen) â†’ fineâ€‘tune with lower LR.  
- **Callbacks:** EarlyStopping, ModelCheckpoint, ReduceLROnPlateau.  
- **Input:** 224Ã—224 RGB, `[0,1]`, with inâ€‘model augmentation.

---

## ğŸ“Š Evaluation Artifacts (examples)

- `classification_report.csv` (optional export from Chunk 4)  
- `confusion_matrix.png` and `confusion_matrix_normalized.png` (optional save)  
- Misclassification gallery figure(s)

> You can add a small save snippet around the plotting functions to persist figures into `./export/` for Git commits.

---

## ğŸ” Explainability (Gradâ€‘CAM)

- Robust Gradâ€‘CAM implementation that computes gradients over the **backbone feature map** within a single graph.  
- **Gallery** for random test images and **singleâ€‘image** function for quick inspection.  
- Use to validate that the model attends to petals/disc florets rather than background.

---

## ğŸš€ Deployment

### SavedModel
```python
best_model.export("export/savedmodel")  # or best_model.save(..., save_format="tf")
```

### Build a fresh inferenceâ€‘only model (no augmentation), copy weights
> See Chunk 6 cell â€œBuild fresh inferenceâ€‘only modelâ€. This prevents conversion issues and matches training preprocessing.

### TFLite conversion (robust)
- FP32 and float16 models saved to `export/model_fp32.tflite` and `export/model_float16.tflite`.  
- The converter attempts: **builtins â†’ SELECT_TF_OPS â†’ concrete function** automatically.

### CLI (TFLite)
```bash
python scripts/flower_cli.py path/to/image.jpg --topk 3   --tflite export/model_float16.tflite
```

### Gradio App (optional)
```python
# in notebook
demo.launch(share=False)
```

---

## âš ï¸ Troubleshooting (What we fixed)

- **LR schedule vs `ReduceLROnPlateau`:** created optimizer with **float LR** (not a schedule) so the callback can adjust it.  
- **Gradâ€‘CAM `KeyError` / graph issues:** rebuilt forward pass inside the CAM function; ensured a single, connected graph; added a fallback gradient path.  
- **TFLite converter errors:** created a **fresh inferenceâ€‘only** model (no augmentation), copied weights, and added **robust converter** fallbacks (SELECT_TF_OPS / concrete function).  
- **Input rank mismatch:** avoided reusing layer instances in a new graph; built a fresh model to prevent extra batch dims.

---

## âœ… Deliverables Checklist

- [ ] Notebook with Chunks 1â€“7 (`notebooks/FlowerClassification.ipynb`)  
- [ ] Trained model: `models/best_model.keras`  
- [ ] Label maps: `models/id2label.json`, `models/label2id.json`  
- [ ] SavedModel export: `export/savedmodel/`  
- [ ] TFLite models: `export/model_fp32.tflite`, `export/model_float16.tflite`  
- [ ] CLI: `scripts/flower_cli.py`  
- [ ] (Optional) Gradio UI ready to launch

---

## ğŸ“ˆ Results (fill with your run)

- Test Accuracy: `â€¦`  
- Topâ€‘3 Accuracy: `â€¦`  
- Macro F1: `â€¦`  

Include a confusion matrix and a few Gradâ€‘CAM overlays demonstrating correct focus.

---

## ğŸ“š Acknowledgments

- Dataset: Kaggle â€” **Flowers Recognition** by *alxmamaev*.  
- Backbones: MobileNetV2, EfficientNetB0 (TensorFlow/Keras Applications).

---

## ğŸ” License

This repository is for coursework (**Assignment 1**). For the dataset, follow Kaggleâ€™s terms of use.

---

### Appendix: Minimal Endâ€‘toâ€‘End Script (outline)

If you later want to script this outside a notebook, the flow is:
1) Data indexing â†’ splits â†’ tf.data pipelines.  
2) Build model â†’ warmâ€‘up â†’ fineâ€‘tune â†’ save artifacts.  
3) Evaluate (report + confusion matrices).  
4) Build inferenceâ€‘only model â†’ export SavedModel â†’ convert to TFLite.  
5) Inference via CLI/Gradio.
